## Стратегия обновления Blue/Green
Сине-зеленое развертывание **Kubernetes** (иногда называемое красным/черным) предполагает развертывание новой версии вместе со старой. Внутренняя команда QA может тестировать новую версию (синий цвет), в то время как клиент продолжает получать доступ к старой версии (зеленый цвет). После тестирования и утверждения релиза трафик можно перенаправить на новую версию (синий цвет), изменив сервис Kubernetes. После этого, когда разработчики будут уверены в работоспособности релиза, старая версия (зеленая) может быть уменьшена до нуля.

![Kubernetes Deployments](./assets/k8s-deployments-blue-green.gif)
Эта стратегия также гарантирует отсутствие простоев в работе клиентов во время развертывания, а также дает бизнесу дополнительный уровень уверенности в новом сервисе (по результатам QA-тестирования). Синяя/зеленая стратегии предполагают больше инженерных работ и являются самыми затратными с точки зрения ресурсов (удвоение ресурсов приложения), однако при необходимости старые версии могут быть немедленно развернуты.
Развертывание по схеме "сине-зеленый" является затратным, поскольку требует удвоения ресурсов. Прежде чем запускать платформу в промышленную эксплуатацию, необходимо провести надлежащее тестирование всей платформы. Кроме того, сложно работать с приложениями, имеющими состояние.

Сначала создадим нашу синюю установку, сохранив следующий yaml в файл 'blue.yaml':
<pre class="file" data-filename="./blue.yaml" data-target="insert">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue-deployment
spec:
  selector:
    matchLabels:
      app: blue-deployment
      version: nginx-124
  replicas: 3
  template:
    metadata:
      labels:
        app: blue-deployment
        version: nginx-124
    spec:
      containers:
        - name: blue-deployment
          image: nginx:1.24
</pre>
И применим манифест

`kubectl apply -f blue.yaml`{{execute T1}}

Состояние деплоймента можно получить с помощью команд:
`kubectl get deploy blue-deployment `{{execute T1}}

Более детальная информация:
`kubectl describe deploy blue-deployment `{{execute T1}}

Далее мы зададим эти метки в качестве селектора меток для сервиса. Сохраним это в файле service.yaml.
<pre class="file" data-filename="./service.yaml" data-target="insert">
apiVersion: v1
kind: Service
metadata: 
  name: blue-green-service
  labels: 
    name: blue-deployment
    version: nginx-124
spec:
  ports:
    - name: http
      port: 80
      targetPort: 80
  selector: 
    name: blue-deployment
    version: nginx-124
  type: LoadBalancer
</pre>
Теперь при создании службы будет создан балансировщик нагрузки, доступный вне кластера.
`kubectl apply -f service.yaml`{{execute T1}}

Для зеленого развертывания мы развернем новое развертывание параллельно с синим развертыванием. Приведенный ниже шаблон является содержимым файла green.yaml:
<pre class="file" data-filename="./green.yaml" data-target="insert">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-deployment
spec:
  selector:
    matchLabels:
      app: green-deployment
      version: nginx-125
  replicas: 3
  template:
    metadata:
      labels:
        app: green-deployment
        version: nginx-125
    spec:
      containers:
        - name: green-deployment
          image: nginx:1.25
</pre>

Применим манифест
`kubectl apply -f green.yaml`{{execute T1}}

Давайте посмотрим общие результаты работы по подготовке приложения в **Kubernetes** запустим команду:

`kubectl get pods,deployments,service`{{execute}}

Далее для того чтобы перейти в режим green deployment, мы должны обновить селектор для существующего сервиса. Изменим service.yaml и поменяем версию селектора на 2, а имя на green-deployemnt. Таким образом, он будет соответствовать **Pods** в green развертывании.
<pre class="file" data-filename="./service.yaml" data-target="insert">
apiVersion: v1
kind: Service
metadata: 
  name: blue-green-service
  labels: 
    name: green-deployment
    version: nginx-125
spec:
  ports:
    - name: http
      port: 80
      targetPort: 80
  selector: 
    name: green-deployment
    version: nginx-125
  type: LoadBalancer
</pre>
Теперь при создании службы будет создан балансировщик нагрузки, доступный вне кластера.
`kubectl apply -f service.yaml`{{execute T1}}

Давайте посмотрим общие результаты работы по подготовке приложения в **Kubernetes** запустим команду:

`kubectl get pods,deployments,service`{{execute}}

Видим подготовленный сервис *LoadBalancer*, а также нобходимое количество **Pods** непосредственно для старой и новой версии приложения.

После переключения всего трафика на новую версию, старые экзмепляры можно будет выодить из эксплуатации.
В заключение можно отметить, что развертывание по принципу "сине-зеленого" - это "все или ничего", в отличие от развертывания по принципу скользящего обновления, при котором мы не можем постепенно распространять новую версию. В этом случае все пользователи получат обновление одновременно, хотя существующим сессиям будет позволено завершить работу на старых экземплярах.

Теперь можем удалить *деплоймент*:

`kubectl delete -f blue.yaml`{{execute T1}}
`kubectl delete -f green.yaml`{{execute T1}}

Вместе с удалением *деплоймента* будут удалены все *поды*.

## Выводы о стратегии обновления Blue/Green
**Плюсы:**
- Мгновенное развертывание/откат.
- Отсутствие проблемы с версионностью, все состояние приложения изменяется за один раз.

**Минусы:**
- Дороговизна, поскольку требует удвоения ресурсов.
- Перед запуском в промышленную эксплуатацию необходимо провести надлежащее тестирование всей платформы.
- Работа с приложениями, имеющими состояние, может оказаться сложной.

## Стратегия обновления Canary
Развертывания **Canary** во многом схожи с развертываниями **blue/green**, с той лишь разницей, что новая версия приложения выпускается для небольшого подмножества клиентов. При этом небольшой процент производственного трафика приложения направляется на новую версию, чтобы разработчики могли наблюдать за развертыванием, пока оно тестируется на реальных клиентах. После того как разработчики будут удовлетворены работой новой версии, они могут начать постепенно увеличивать объем трафика, направляемого на новую версию, пока весь трафик не будет направлен на новую версию. После этого старая версия уменьшается до нуля.

![Kubernetes Deployments](./assets/k8s-deployments-canary.gif)

Канареечное развертывание заключается в постепенном переводе производственного трафика с версии А на версию В. Обычно трафик делится по весу. Например, 90% запросов поступает на версию A, 10% - на версию B.
Этот метод чаще всего используется в тех случаях, когда тесты недостаточны или ненадежны, а также если нет уверенности в стабильности нового релиза на платформе.

При планировании развертывания **Canary** необходимо учитывать различные моменты:
- Этапы: сколько пользователей и сколько этапов мы планируем запустить "канарейку".
- Продолжительность: как долго мы планируем использовать canary? Релизы canary отличаются тем, что нам необходимо дождаться обновления достаточного количества клиентов, прежде чем мы сможем оценить результаты. Это может происходить в течение нескольких дней или даже недель.
- Метрики: какие метрики необходимо регистрировать для анализа прогресса, включая производительность приложения и отчеты об ошибках? Хорошо подобранные параметры очень важны для успешного развертывания канарейки. Например, очень простым способом оценки развертывания являются коды состояния HTTP. Мы можем иметь простую службу ping, которая при успешном развертывании возвращает 200. При возникновении проблем с развертыванием он будет возвращать ошибку конца сервера (5xx).
- Оценка: какие критерии мы будем использовать для определения успешности работы канарейки

Наш первый файл, stable.yaml, будет представлять собой устаревшую версию, на которой будет работать большинство наших подсистем.
<pre class="file" data-filename="./stable.yaml" data-target="insert">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: helloworld
spec:
  selector:
    matchLabels:
      app: helloworld
  replicas: 3
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 5
  template:
    metadata:
      labels:
        app: helloworld
        track: stable
    spec:
      containers:
      - name: helloworld
        image: educative/helloworld:1.0
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
          limits:
            cpu: 100m
</pre>

В результате будет создано 3 Pods v1 с меткой app:helloworld, которую ищет наш сервис Kubernetes. Наш образ для этих Pod'ов - educative/helloworld:1.0, что означает, что эти Pod'ы будут созданы на основе старых спецификаций Pod'ов.

Это развертывание равномерно распределит любую рабочую нагрузку между доступными Pod.

Для развертывания необходимо ввести в командную строку следующую строку:
`kubectl apply -f stable.yaml`{{execute T1}}

Состояние деплоймента можно получить с помощью команд:

`kubectl get deploy`{{execute T1}}

Теперь создадим новый манифест для новой версии приложения
<pre class="file" data-filename="./canary.yaml" data-target="insert">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: helloworld-canary
spec:
  selector:
    matchLabels:
      app: helloworld
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  minReadySeconds: 5
  template:
    metadata:
      labels:
        app: helloworld
        track: canary
    spec:
      containers:
      - name: helloworld
        image: educative/helloworld:2.0
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 50m
          limits:
            cpu: 100m
</pre>

Для этого развертывания мы создадим только один **Pod** (строка 6), чтобы обеспечить взаимодействие большинства наших пользователей с *v1*.
Оба развертывания сбалансируют рабочую нагрузку между всеми **Pod**, что гарантирует, что только 25% нашей рабочей нагрузки будет приходиться на обновленный Pod.

Для развертывания необходимо ввести в командную строку следующую строку:
`kubectl apply -f canary.yaml`{{execute T1}}

Состояние деплоймента можно получить с помощью команд:

`kubectl get deploy`{{execute T1}}

Когда убедились, что *v2* работает, просто заменим образ в нашем первом YAML-файле Deployment, stable.yaml, на educative/helloworld:2.0, вместо educative/helloworld:1.0.

Затем удалите canary Deployment с помощью:
`kubectl delete -f canary.yaml`{{execute T1}}

В этом случае все **Pod** будут иметь *v2*, а нагрузка будет сбалансирована между оставшимися 3 **Pod**, но уже с *v2* приложения.

Обновление Canary достигнуто!

## Выводы о стратегии обновления Canary

**Плюсы:**
- Версия, выпущенная для подмножества пользователей.
- Удобна для мониторинга количества ошибок и производительности.
- Быстрый откат.

**Минусы:**
- Замедленное развертывание.

Две оставшиеся стратегии обновления рассмотрим тезисно.

## Стратегия обновления A/B Testing
A/B-тестирование подразумевает проведение тестирования подмножества пользователей на предмет использования новой функциональности при определенных условиях. Обычно это не стратегия развертывания, а метод принятия бизнес-решений на основе статистики. Тем не менее, это связано и может быть реализовано путем добавления дополнительной функциональности в канареечное развертывание.

![Kubernetes Deployments](./assets/k8s-deployments-a-b.gif)

Эта техника широко используется для тестирования конверсии той или иной функции и запуска только той версии, которая дает наибольшую конверсию.
Ниже приведен список условий, которые могут быть использованы для распределения трафика между версиями:
- По cookie браузера
- Параметры запроса
- Геолокация
- Технологическая поддержка: версия браузера, размер экрана, операционная система и т.д.
- Язык

## Выводы о стратегии обновления A/B Testing
**Плюсы:**
- Параллельная работа нескольких версий.
- Полный контроль над распределением трафика.

**Минусы:**
- Требуется интеллектуальный балансировщик нагрузки.
- Сложность поиска ошибок для конкретного сеанса, обязательной становится распределенная трассировка.

## Стратегия обновления Shadow
Теневая стратегия предполагает развертывание новой версии параллельно со старой и последующую переброску реального трафика клиентов из старой версии в новую. Это позволяет протестировать новую версию на реальном клиентском трафике без ущерба для клиента. Полное развертывание новой версии начинается, когда стабильность и производительность соответствуют требованиям.

![Kubernetes Deployments](./assets/k8s-deployments-shadow.gif)

Эта методика достаточно сложна в настройке и требует особых требований, особенно при работе с исходящим трафиком. Например, на платформе корзины покупок при теневом тестировании платежного сервиса можно столкнуться с тем, что клиент дважды оплатит свой заказ. В этом случае можно решить проблему, создав имитационный сервис, который будет копировать ответ от платежной системы.

## Выводы о стратегии обновления Shadow
**Плюсы:**
- Тестирование производительности приложения с производственным трафиком.
- Отсутствие воздействия на пользователя.
- Развертывание не производится до тех пор, пока стабильность и производительность приложения не будут соответствовать требованиям.

**Минусы:**
- Затратно, поскольку требует удвоения ресурсов.
- Не является настоящим пользовательским тестированием и может вводить в заблуждение.
- Сложность в настройке.
- Для некоторых случаев требуется использование mocking-сервиса.
